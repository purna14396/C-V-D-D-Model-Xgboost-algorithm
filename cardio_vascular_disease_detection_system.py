# -*- coding: utf-8 -*-
"""Cardio Vascular disease detection system.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vjg3XKtObYtXHhr1iCaKds5G_jMcLusl

# **Cardio Vascular disease detection**

This project demonstrates how to use an XGBoost classifier to predict whether an individual is likely to have cardiovascular disease based on various features like age, gender, blood pressure, cholesterol, etc. The larger dataset is preprocessed, split into training and test sets, and an XGBoost model is trained. After training, the model is used to make predictions based on user inputs.


## **Steps to Build the Model:**

# **1. Import Required Libraries**
We first import the necessary Python libraries for data manipulation, visualization, and model building.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import xgboost as xgb

"""# **2. Load and Preprocess the Data**
We load the dataset and perform basic data cleaning tasks, such as removing unnecessary columns, converting age from days to years, mapping the gender values, and handling missing values.
"""

# Load the CSV data
data = pd.read_csv('/content/sample_data/cardio_dataset.csv')

# Drop the 'id' column as it is not required for modeling
data.drop(columns=['id'], inplace=True)

# Convert age from days to years
data['age'] = data['age'] // 365

# Convert gender to categorical (0: Female, 1: Male)
data['gender'] = data['gender'].map({1: 0, 2: 1})

# Check for and handle any missing values (if present)
data.dropna(inplace=True)

"""# **3. Feature Selection and Target Definition**
We split the data into features (X) and the target variable (y). The target variable cardio indicates whether an individual has cardiovascular disease.
"""

# Split the data into features (X) and target (y)
X = data.drop(columns=['cardio'])
y = data['cardio']

"""# **4. Data Visualization**
To better understand the data, we compute basic statistics, plot the correlation matrix to understand relationships between features, and visualize the distributions of numerical features like age, height, weight, etc.
"""

# Show basic statistics of the dataset
print(data.describe())
print()

# Show correlation matrix
correlation_matrix = data.corr()
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()
print()

# Plot distributions of numerical features
num_features = ['age', 'height', 'weight', 'ap_hi', 'ap_lo']
for feature in num_features:
    sns.histplot(data[feature], kde=True)
    plt.title(f'Distribution of {feature}')
    plt.show()
print()


'''
# Plot count of each category for categorical features
cat_features = ['cholesterol', 'gluc', 'smoke', 'alco', 'active']
for feature in cat_features:
    sns.countplot(data[feature])
    plt.title(f'Count of {feature}')
    plt.show()
print()
'''

"""# **5. Data Splitting and Scaling**
We split the data into training and testing sets using an 80/20 split. We also scale the data using StandardScaler to ensure that all features have the same scale.
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

"""# **6. Model Building with XGBoost**
We use the XGBClassifier from the xgboost library to build the model and train it on the training data.
"""

# Build the XGBoost model
model = xgb.XGBClassifier()
model.fit(X_train, y_train)

"""# **7. Model Evaluation**
After training, we evaluate the model by predicting the results on the test set. We calculate the accuracy, print a classification report, and display the confusion matrix.
"""

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f'\nAccuracy: {accuracy:.2f}')

# Print classification report and confusion matrix
print('\nClassification Report:')
print(classification_report(y_test, y_pred))

print('\nConfusion Matrix:')
print(confusion_matrix(y_test, y_pred))

print()

"""# **8. User Input for Prediction**
We gather inputs from the user regarding their age, gender, height, weight, blood pressure, cholesterol levels, etc., to make predictions for their cardiovascular risk.
"""

# Get user input to make predictions
user_input = {
    'age': int(input('Enter your age in years: ')),
    'gender': int(input('Enter your gender (0: Female, 1: Male): ')),
    'height': float(input('Enter your height in centimeters: ')),
    'weight': float(input('Enter your weight in kilograms: ')),
    'ap_hi': int(input('Enter your systolic blood pressure: ')),
    'ap_lo': int(input('Enter your diastolic blood pressure: ')),
    'cholesterol': int(input('Enter your cholesterol level (1: normal, 2: above normal, 3: well above normal): ')),
    'gluc': int(input('Enter your glucose level (1: normal, 2: above normal, 3: well above normal): ')),
    'smoke': int(input('Do you smoke? (0: No, 1: Yes): ')),
    'alco': int(input('Do you consume alcohol? (0: No, 1: Yes): ')),
    'active': int(input('Are you physically active? (0: No, 1: Yes): '))
}
print()

"""# **9. Convert and Scale User Input**
We convert the user input, scale it using the StandardScaler, and make a prediction.
"""

# Prepare the user input for prediction
user_data = pd.DataFrame(user_input, index=[0])
user_data['age'] = user_data['age'] // 365
user_data['gender'] = user_data['gender'].map({1: 0, 2: 1})
user_data = scaler.transform(user_data)

"""# **9. Make Predictions on new data**
Prediction the new data
"""

# Make prediction
prediction = model.predict(user_data)[0]
print()
if prediction == 0:
    print("Congratulations! You are predicted to be free of cardiovascular disease.")
else:
    print("You are predicted to have cardiovascular disease. Please consult a doctor for further evaluation.")